= Replication
:sourcedir: replication-demo

Replicating a service can provide many benefits. In this section we will
replicate a http://postgresql.org[PostgreSQL] database service to analyze the
advantages and complexity costs. PostgreSQL was purposefully chosen because it
leaves much of work, which is understandably outside the purview of a Database
Management System (DBMS), to the user. We will be using Docker compose so that
we do not have to introduce a new tool as well.

== Background

.Basic Replication
[plantuml, "replication", svg]
....
@startuml

node Database {
    database db2
    database db1
}

@enduml
....

In the simplest sense, replication is running more than one service for a given
component of our system. We can do this in Docker Compose by declaring more
than one service:

.{sourcedir}/docker-compose.yml (excerpted)
[source, YAML]
----
include::{sourceroot}/{sourcedir}/docker-compose.yml[tags=db_services]
----

NOTE: A better solution would be to use a more complete orchestration solution
than Docker Compose. For syntax that you are used to, see
https://docs.docker.com/compose/compose-file/#deploy[the deploy option and
Docker Swarm]. You could also bite the bullet and migrate to
https://kubernetes.io[kubernetes].

In most DBMS, replication can be implemented via Volume Sharing or Hot / Warm
Standby:

.Volume Sharing
[plantuml, volshare, svg]
....
@startuml

@startuml

actor User
node Database {
    database db2 [
        db2 (RW)
    ]
    database db1 [
        db1 (RW)
    ]
}

node Network [
    <b>Network Storage
    ----
    NFS
    ....
    AFS
    ....
    GlusterFS
    ....
    DRBD
    ....
    Ceph
    ....
    S3
    ....
    . . .
]

node Volumes [
    <b>Volumes
    ----
    data
    ....
    backup
    . . .
]

User -> Database
db1 --> Network
db2 --> Network
Network -> Volumes

@enduml
....

.Hot / Warm Standby
[plantuml, standby, svg]
....
@startuml

actor User
node Database {
    database db2 [
        db2 (Standby R)
    ]
    database db1 [
        db1 (Primary RW)
    ]
}
node Volumes {
    node data1
    node data2
}

User -> Database
db1 -> db2: log records
db1 --> data1
db2 --> data2

@enduml
....

Volume Sharing has the advantage of being easy to implement _if_ you already
have network storage. You can scale simply by increasing the amount of
database instances (db3, db4, db5...). Each instance has full read / write
access, making it easy to load balance. Despite that, reading / writing from
network storage tends to be a bottleneck. Also, shared access to files and the
issues that arise (file locking, etc.) are difficult problems. Your DBMS may
not be able to handle them. Even if they can be handled, performance can be an
issue. Lastly, this method puts all of your eggs in one basket with regard to
where your data is stored. There may be no duplicates of the data depending on
how you handle your network storage.

Hot / Warm Standby mode is typically already implemented by the DBMS. It is
usually performed via _Log Shipping_, sending logs of all actions between a
primary node and standby nodes. The standby nodes can keep up with transactions
and be _promoted_ in the event of an issue. Each node maintains a separate data
volume. With this system, only the primary node is capable of performing write
operations. If the standby node is a _hot_ standby node it can perform reads as
well. Fortunately, write operations tend to be less frequent that read
operations, meaning you may see significant performance gains from scaling with
this type of system.

NOTE: What's the difference between a hot and warm standby? A hot standby can
perform read operations while replicating the primary. This allows for load
balancing to be performed. A warm standby simply keeps up with the primary so
that it can be brought up in the event of a problem.

== Implementation

This example implements a
https://www.postgresql.org/docs/current/hot-standby.html[Hot Standby in PostgreSQL].
Actually implementing _replication_ is largely handled for us by the DMBS. It is
simply a matter of setting configuration variables, starting the databases are
in the correct state, and bringing up the services in the right order. We can
handle this in the `docker-entrypoint.sh` file that is used as the ENTRYPOINT
for the container. In the https://hub.docker.com/_/postgres[official Docker
image] this script is responsible for setting up the database and running the
user-specified command, `postgres` by default.

A basic Dockerfile that builds from this image and adds some extra needed
utilities will be used:

.{sourcedir}/db/Dockerfile
[source, Dockerfile]
----
include::{sourceroot}/{sourcedir}/db/Dockerfile[]
----

Rather than create separate primary and standby images, this example employs one
image that detects the status of the cluster on startup and creates either a
primary or standby node accordingly. The logic for startup is as follows:

[plantuml, startup_logic, svg]
....
@startuml

"Start" --> if "Is there a PRIMARY node?" then
    -->[yes] "Become a STANDBY"
else
    -->[no] if "Are we first in the node list?" then
        -->[yes] "Become a PRIMARY"
    else
        -->[no] "Wait 10 seconds"
        --> "Search again for a PRIMARY node"
        if "Is there a PRIMARY node now?" then
            -->[yes] "Now become a STANDBY"
        else
            -->[no] "Now become a PRIMARY"
        endif
    endif
endif

@enduml
....

By passing a list of all nodes in an environment variable, we can create a bash
function to see who is up and who is the primary when `docker-entrypoint.sh` is
called at container creation:

.{sourcedir}/db/docker-entrypoint.sh (excerpted)
[source, bash]
----
include::{sourceroot}/{sourcedir}/db/docker-entrypoint.sh[tags=find_primary]
----

NOTE: If a situation arises where there are two primaries on the network, this
function will prevent a new db container from being created. No sense adding to
the confusion.

Now all that is left is to configure either a primary or a standby node.
The code to configure a primary node follows:

.{sourcedir}/db/docker-entrypoint.sh (excerpted)
[source, bash]
----
include::{sourceroot}/{sourcedir}/db/docker-entrypoint.sh[tags=configure_primary]
----

The code is self-explanatory, but it should be noted that the actual building
of the database, including auth configuration, and the create of a replication
user is a rare occurrence. That should only happen once when the volume is
initialized.

The code to configure a standby node is shown below:

.{sourcedir}/db/docker-entrypoint.sh (excerpted)
[source, bash]
----
include::{sourceroot}/{sourcedir}/db/docker-entrypoint.sh[tags=configure_standby]
----

When a standby is brought up, any database on the volume is removed and the
entire database from the primary is backed up You may want to revisit this
design decision if your database becomes large. Connection info is also added to
the end of `postgres.conf`. This does mean that if a standby is promoted the
previous connect line will be synced to any new standbys that are brought up.
This will result in an ever growing `postgres.conf` file. The monitor function
will be covered in the next section.

NOTE: You may notice that primaries and standbys have a very similar
configuration. This is by design in PostgreSQL >= 12, to simplify the failover
procedure.

Let's take a look at the relevant log messages of db1 and db2 when we bring them
both up with `docker-compose up`:

[source, shell]
----
db1 - Looking for a primary node...
db1 - db2:5432 - no response
db1 - Configuring a PRIMARY instance...
db2 - Looking for a primary node...
db2 - db1:5432 - no response
db2 - Giving the first node a 10s head start...
db2 - Looking for a primary node...
db2 - db1:5432 - accepting connections
db2 - db1 is primary node
db2 - Configuring a STANDBY instance...
----

As can be seen, db2 wasn't able to detect db1 at first but the additional 10s
delay prevented a multiple-primary situation.

WARNING: Having multiple primaries in a cluster is bad. So bad, that most
solutions implement a https://en.wikipedia.org/wiki/STONITH[STONITH] policy. It
is quite possibly the greatest acronym in all of technology.

WARNING: Just because we have our database replicated on two volumes *does not*
mean that we have backups. Those volumes are designed to be used as part of a
running system and are not a reliable long-term solution. Create and implement
a reliable backup plan as you would for any other database.

== High Availability

Even though we now have a replicated database, it isn't doing us much good. If
we want to make our database service highly available (HA) we will need to
monitor for problems and promote a standby server if the primary server fails.
This is referred to as _failover_ and is often handled by a
https://wiki.clusterlabs.org/wiki/Pacemaker[a separate component].  In this
simple example it is implemented in the `monitor` function shown below:

.{sourcedir}/db/docker-entrypoint.sh (excerpted)
[source, bash]
----
include::{sourceroot}/{sourcedir}/db/docker-entrypoint.sh[tags=monitor]
----

This function is run in the background on every standby instance. The
timeout is randomized to decrease the likelihood that two standby instances
will promote themselves at exactly the same time. To better understand this,
let's examine the *non-randomized* scenario shown in the following diagram:

.Dual Promotion
[plantuml, dualpromotion, svg]
....
@startuml
concise "Node 1" as N1
concise "Node 2" as N2
concise "Node 3" as N3

@0
N1 is Primary
N2 is Standby
N3 is Standby

@10

@20

@30
N2 is Check
N3 is Check

@40
N2 is Standby
N3 is Standby

@50
N1 is Failed

@60
N2 is Check
N3 is Check

@70
N2 is Primary
N3 is Primary

@80

@enduml
....

In the above scenario each standby node is checking to see that there is a
primary node every 30s. The primary fails at 50s and _both_ nodes check for a
primary at exactly 60s. At that moment, neither node is a primary, no primary
can be found, and they both begin the promotion process. This leaves the
cluster with two primary nodes. This can be largely avoided by randomizing
the check intervals.

Finally, lets simulate a failure and a recovery to see that our HA system is
working.  The following commands were executed and the Docker Compose logs were
captured.  Each command was run with about a minute pause between them:

. `docker-compose up`
. `docker-compose stop db1`
. `docker-compose up db1`

The relevant, simplified log messages and descriptions follow:

[source, shell]
----
db2 - Looking for a primary node...
db1 - Looking for a primary node...
db2 - db1:5432 - no response
db1 - db2:5432 - no response
db2 - Giving the first node a 10s head start...
db1 - Configuring a PRIMARY instance...
db2 - Looking for a primary node...
db2 - db1:5432 - accepting connections
db2 - db1 is primary node
db2 - Configuring a STANDBY instance...
----

Both db1 and db2 are brought up at the same time. db1 ends up being
the primary.

[source, shell]
----
db2 - Looking for a primary node...
db2 - db1:5432 - accepting connections
db2 - db1 is primary node
db2 - Looking for a primary node...
db2 - db1:5432 - accepting connections
db2 - db1 is primary node
----

db1 begins checking to make sure there is a primary node about every 30s.

[source, shell]
----
db1 - LOG:  shutting down
db2 - Looking for a primary node...
db2 - db1:5432 - no response
db2 - Can't find a primary failing over...
db2 - server promoted
----

db1 is shut down. db2 performs its regularly scheduled check and self promotes
because it cannot find a primary.

[source, shell]
----
db1 - Looking for a primary node...
db1 - db2:5432 - accepting connections
db1 - db2 is primary node
db1 - Configuring a STANDBY instance...
db1 - Looking for a primary node...
db1 - db2:5432 - accepting connections
db1 - db2 is primary node
db1 - Looking for a primary node...
db1 - db2:5432 - accepting connections
db1 - db2 is primary node
----

db1 is brought back up, finds another primary and makes itself a standby. db1
begins checking to make sure the primary is available at regular intervals.

== Load Balancing

Another advantage to replication is the ability to split the work among
different nodes in the cluster. In our particular case, the primary node can
handle any request, while the standby node can only handle read requests. Since
we also control the application, we could create a connection for read requests
and a separate connection for write requests.

Fortunately this can be accomplished by
https://www.percona.com/blog/2019/10/23/seamless-application-failover-using-libpq-features-in-postgresql/[changing
the connect string that is used] in your application:

[source, python]
----
# get a read / write connection
psycopg.connect(
    database="example",
    host="db1,db2",
    user="postgres",
    password=password,
    target_session_attrs="read-write"
)

# get a read connection
psycopg.connect(
    database="example",
    host="db2,db1", # order should be randomized
    user="postgres",
    password=password,
    target_session_attrs="any"
)
----

The other option is to employ a proxy to forward requests, the most popular
being http://www.haproxy.org/[HAProxy].

TIP: Load balancing is often referenced as a part of _horizontal scaling_.
You can think of horizontal scaling as adding more instances to serve requests.
_Vertical scaling_ refers to making instances more powerful so that each
individual one can serve more requests.

== Questions

[qanda]
What are some of the issues with a volume sharing database?::
    {empty}
Why does our example have a startup delay? What could happen if we brought all of the nodes up at the same time?::
    {empty}
What is the difference between high availability and load balancing?::
    {empty}
What does a _hot_ standby node do?::
    {empty}
Our example starts up two nodes, but what would we have to change in our docker-compose.yml file if we wanted to start ten nodes? Is this congruent with the https://en.wikipedia.org/wiki/Don%27t_repeat_yourself["Don't Repeat Yourself" (DRY)] principle?::
    {empty}
