= Front End in Kubernetes
:sourcedir: example-final

== Introduction

Migrating *Front End* to Kubernetes should be a relatively simple. It is already
designed with horizontal scaling in mind. All communication with the other
components is handled by *Messaging* and *Front End* does not maintain any
state. Multiple *Front Ends* can be run at the same time and as far as the
client is concerned, any instance can be used. Kubernetes *Services* can be used
to manage our *Messaging* and *Front End* instances, making connections easy:

[plantuml, front-end-k8s, svg]
....
@startuml
rectangle "Front End" as frontend {
    node "front-end1"
    node "front-end2"
    node "front-end3"
}

rectangle Messaging {
    node messaging1
    node messaging2
    node messaging3

    messaging1 <-> messaging2
    messaging1 <--> messaging3
    messaging2 <--> messaging3
}

frontend <--> Messaging

@enduml
....

NOTE: Notice how the front-end instances don't need to communicate with each
other unlike the messaging instances which are part of a cluster. This makes
scaling much less complex.

== Kubernetes

*Front End* requires a way of accessing a *Service* from the outside world. The
traditional way of doing this is through a load balancer, which has an external
IP and forwards traffic from a standard port, 80 for HTTP or 443 for HTTPS, to
the *Service* and ultimately one of the running pods. The concept of load
balancing isn't new to us, we have been using it implicitly when we create a
*Service*. The new concept is a method of external access.

.Load Balancer Architecture
[plantuml, loadbalancer, svg]
....
@startuml

actor User
node loadbalancer [
    <b>Load Balancer
    (NodePort for minikube)
    ----
    External IP
]
node service [
    <b>Service
    ----
    Cluster IP (internal)
]
node pod1 [
    <b>Pod 1
    ----
    Individual IP (internal)
    ----
    front-end Container
]
node pod2 [
    <b>Pod 2
    ----
    Individual IP (internal)
    ----
    front-end Container
]
node pod3 [
    <b>Pod 3
    ----
    Individual IP (internal)
    ----
    front-end Container
]

User -> loadbalancer: Port 80/443 (30000-32767 for minikube)
loadbalancer --> service: Kubernetes API
service --> pod1: Port 5000
service --> pod2: Port 5000
service --> pod3: Port 5000

@enduml
....

Our Flask application will listen on port 5000 by default. In an actual
production environment (not minikube) the load balancer would be given an
external IP listening on a standard port. Minikube will support the definition
of a LoadBalancer type *Service* object, but it will actually use a slightly
simpler object called a
https://kubernetes.io/docs/concepts/services-networking/service/#nodeport[
NodePort] to access the service. For us, this means we can get to our service
via a local IP and a random port between 30000 and 32767. The
`minikube service` command will automatically open the URL for the service in
your default web browser.

== Example

=== Updating the Docker Image

The Docker image for use in Kubernetes can actually be simplified from what we
were running before. We can remove the `wait-for-it.sh` script and we no longer
have to call it from the Dockerfile:

.{sourcedir}/front-end/Dockerfile
[source, Dockerfile]
----
include::{sourceroot}/{sourcedir}/front-end/Dockerfile[]
----

Our *Service* name for *Messaging* is still messaging, so we don't even need
to change the hostname the Messaging class connects to. If you look at the
source code you will see that I just changed the comment to reference
Kubernetes instead of Docker Compose.

=== Building the Docker Image

In order for Kubernetes to be able to use our custom image, we need to build it
and make it available to the Docker daemon running _inside_ minikube. With
minikube started, but without the environment set up correctly, `docker ps`
will only show the containers you have running natively on the host:

[source, shell]
----
PS example-final> docker ps
CONTAINER ID  IMAGE                                COMMAND               
9689f05c2fca  gcr.io/k8s-minikube/kicbase:v0.0.8   "/usr/local/bin/entr…" 
----

NOTE: In this example the only container I have running is the container used
by the minikube `docker` driver. If you are running it under `virtualbox` or
`hyperv` you may not see any containers running. If you don't have docker
running on your host, you may not even be able to execute the `docker ps`
command.

Now if we execute the `minikube docker-env` command and follow the directions,
`docker ps` should show the entire Kubernetes environment running in containers
as it is querying the Docker daemon running _inside_ minikube:

[source, shell]
----
PS example-final> minikube docker-env <1>
$Env:DOCKER_TLS_VERIFY = "1"
$Env:DOCKER_HOST = "tcp://127.0.0.1:32769"
$Env:DOCKER_CERT_PATH = "C:\Users\rxt1077\.minikube\certs"
$Env:MINIKUBE_ACTIVE_DOCKERD = "minikube"
# To point your shell to minikube's docker-daemon, run:
# & minikube -p minikube docker-env | Invoke-Expression <2>
PS example-final> minikube docker-env | Invoke-Expression <3>
PS example-final> docker ps <4>
CONTAINER ID        IMAGE                  COMMAND                  CREATED
47eff1ee88b2        67da37a9a360           "/coredns -conf /etc…"   15 hours ago
79d53aceb8f2        67da37a9a360           "/coredns -conf /etc…"   15 hours ago
c0eebfebded2        aa67fec7d7ef           "/bin/kindnetd"          15 hours ago
b6a95759fdbe        43940c34f24f           "/usr/local/bin/kube…"   15 hours ago
2173eeb16643        k8s.gcr.io/pause:3.2   "/pause"                 15 hours ago
3b72a0aa725b        4689081edb10           "/storage-provisioner"   15 hours ago
4616fd610301        k8s.gcr.io/pause:3.2   "/pause"                 15 hours ago
78e245e291fb        k8s.gcr.io/pause:3.2   "/pause"                 15 hours ago
764d41d70f58        k8s.gcr.io/pause:3.2   "/pause"                 15 hours ago
29f46b453297        k8s.gcr.io/pause:3.2   "/pause"                 15 hours ago
fa87bf3bdcfb        a31f78c7c8ce           "kube-scheduler --au…"   15 hours ago
5e51df5cc257        d3e55153f52f           "kube-controller-man…"   15 hours ago
dc051639dbed        74060cea7f70           "kube-apiserver --ad…"   15 hours ago
01cb4068fc8b        303ce5db0e90           "etcd --advertise-cl…"   15 hours ago
efb620d9f59a        k8s.gcr.io/pause:3.2   "/pause"                 15 hours ago
f723dce3ac9d        k8s.gcr.io/pause:3.2   "/pause"                 15 hours ago
89d58b537cae        k8s.gcr.io/pause:3.2   "/pause"                 15 hours ago
d05cf6dbe82a        k8s.gcr.io/pause:3.2   "/pause"                 15 hours ago
----
<1> Running docker-env by itself prints out how the command should be executed
<2> Here it is telling us how to run it
<3> Now we actually run it as recommended and change the environment
<4> `docker ps` now lists everything running on the minikube docker daemon

Now we can build our front-end image and it will be available to minikube:

[source, shell]
----
PS example-final> docker build -t front-end:v1 ./front-end
Sending build context to Docker daemon  9.728kB
Step 1/6 : FROM python
latest: Pulling from library/python
7e2b2a5af8f6: Pull complete
09b6f03ffac4: Pull complete
dc3f0c679f0f: Pull complete
fd4b47407fc3: Pull complete
b32f6bf7d96d: Pull complete
3940e1b57073: Pull complete
ce1fce2a6cf9: Pull complete
1f593157bb4c: Pull complete
bde1ccd8f1b8: Pull complete
Digest: sha256:3df040cc8e804b731a9e98c82e2bc5cf3c979d78288c28df4f54bbdc18dbb521
Status: Downloaded newer image for python:latest
 ---> b55669b4130e
Step 2/6 : COPY . /app
 ---> b88600cc635a
Step 3/6 : WORKDIR /app
 ---> Running in 20bc72069ed8
Removing intermediate container 20bc72069ed8
 ---> 61eb3608a02a
Step 4/6 : RUN pip install -r requirements.txt
 ---> Running in da9520ffee48
Collecting Flask
  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)
Collecting pika
  Downloading pika-1.1.0-py2.py3-none-any.whl (148 kB)
Collecting itsdangerous>=0.24
  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)
Collecting Werkzeug>=0.15
  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)
Collecting click>=5.1
  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)
Collecting Jinja2>=2.10.1
  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)
Collecting MarkupSafe>=0.23
  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux1_x86_64.whl (32 kB)
Installing collected packages: itsdangerous, Werkzeug, click, MarkupSafe, Jinja2, Flask, pika
Successfully installed Flask-1.1.2 Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.1 itsdangerous-1.1.0 pika-1.1.0
Removing intermediate container da9520ffee48
 ---> 8d2f4da8b8b4
Step 5/6 : ENV FLASK_APP=app.py
 ---> Running in 4cdf7ad5a96e
Removing intermediate container 4cdf7ad5a96e
 ---> 4b5853571124
Step 6/6 : CMD ["flask", "run", "--host=0.0.0.0"]
 ---> Running in ff512bc5e42b
Removing intermediate container ff512bc5e42b
 ---> 52ec5d015433
Successfully built 52ec5d015433
Successfully tagged front-end:v1
----

WARNING: Make sure you give your image a tag with a version ("v1" in our
example). Kubernetes will automatically try to pull the "latest" version
for untagged images and since we are not using a Docker image repository
that pull will fail.

=== Service

Let's take a look at our *Service* definition:

.{sourcedir}/front-end-k8s.yml (excerpted)
[source, YAML]
----
include::{sourceroot}/{sourcedir}/front-end-k8s.yml[tags=service]
----

The only new thing in this definition is `type: LoadBalancer` in the `spec`.
This allows us to access this service externally. It is worth noting that in a
real-life scenario, this will create a load balancer with an external IP via
your IaaS provider. These cost money and it can add up as you expose more
services to the outside world. Fortunately, as explained in the previous
<<_kubernetes_3,Kubernetes>> section, we can still use minikube to test externally connecting
to our service with the `minikube service` command.

=== Deployment

For *Front End* we can use a simple deployment:

.{sourcedir}/front-end-k8s.yml (excerpted)
[source, yml]
----
include::{sourceroot}/{sourcedir}/front-end-k8s.yml[tags=deployment]
----

=== Running the Example

Now let's apply both *Messaging* and *Front End*. This will let us check to see
if the *Front End* serves web pages _and_ if the *Front End* can connect to
*Messaging* and create queues. We won't be able to login / register users
entirely yet because we don't have *Back End* running.

[source, shell]
----
PS example-final> kubectl apply -f messaging-k8s.yml -f front-end-k8s.yml <1>
serviceaccount/messaging created
role.rbac.authorization.k8s.io/rabbitmq-peer-discovery-rbac created
rolebinding.rbac.authorization.k8s.io/rabbitmq-peer-discovery-rbac created
configmap/rabbitmq-config created
service/messaging created
statefulset.apps/messaging created
service/front-end created
deployment.apps/front-end created
PS example-final> kubectl get pod <2>
NAME                         READY   STATUS    RESTARTS   AGE
front-end-7f7c4f5455-6qbcx   1/1     Running   0          4h46m
front-end-7f7c4f5455-htdlv   1/1     Running   0          4h46m
front-end-7f7c4f5455-q2nn6   1/1     Running   0          4h46m
messaging-0                  1/1     Running   0          16m
messaging-1                  1/1     Running   0          16m
messaging-2                  1/1     Running   0          16m
PS example-final> minikube service front-end <3>
|-----------|-----------|-------------|----------------------------|
| NAMESPACE |   NAME    | TARGET PORT |            URL             |
|-----------|-----------|-------------|----------------------------|
| default   | front-end | http/5000   | http://192.168.135.5:31232 |
|-----------|-----------|-------------|----------------------------|
* Opening service default/front-end in default browser...
----
<1> The `kubectl apply` command can be used with multiple files or all YAML
files in a directory. Here we specify the two components we want to start.
<2> After a little while, you should see six pods running: three for
*Messaging* and three for *Front End*.
<3> This command will start the default browser and pass it the URL to the
front-end *Service*. If you just want the URL instead of having it open the
browser you can use `kubectl service --url front-end`.

WARNING: If you apply the objects, run the `kubectl get pod` command, and see
`ErrImagePull` or `ImagePullBackOff` it means that Kubernetes can't pull your
Docker images. Either you've misnamed a stock image that in the `name`
attribute of your container, or you are trying to use a custom image that
you did not make available to minikube. See <<Building the Docker Image>>.

WARNING: If your connections are timing out with the `minikube service` command
and you are using the minikube `docker` driver, try with `hyperv` or
`virtualbox`. The `docker` driver seems to have some issues with port
forwarding.

We see our *Front End* website in our default browser. If we try to register
a user we get a message saying "No response from back end."

TIP: If you want to test things quickly, without opening a browser the
https://curl.haxx.se/[curl] command is a great thing to know. In PowerShell
curl is an alias to Invoke-WebRequest, but it will still work for simple
testing. Try `curl $(minikube service --url front-end)`.

Now let's run a shell in our RabbitMQ cluster and use the `rabbitmqctl` command
to verify that a `requests` queue has actually been created:

[source, shell]
----
PS C:\Users\rxt1077\it490\example-final> kubectl exec -it messaging-0 -- bash <1>
root@messaging-0:/# rabbitmqctl list_queues
Timeout: 60.0 seconds ...
Listing queues for vhost / ...
name    messages
request 1 <2>
root@messaging-0:/# exit
exit
----
<1> It doesn't matter which node we run a shell on, they should all be able to
see all queues. I chose `messaging-0` because it is easy to remember.
<2> There is a request queue, with one message in it.

== Questions

[qanda]
Why is it easier to set up *Front End* in Kubernetes than it is to set up *Messaging*?::
    {empty}
What does a LoadBalancer type *Service* do?::
    {empty}
When creating custom images for use with minikube, why do you have to set up your Docker environment variables before building images?::
    {empty}
How do you make a *Service* accessible from outside the Kubernetes cluster?::
    {empty}
If you wanted to use the RabbitMQ web-based admin interface for testing, what would you have to do to access it?::
    {empty}
